import logging
import multiprocessing
from pathlib import Path

import numpy as np
from tqdm import tqdm

from centreseq.bin.tree.cluster_data_structures import populate_cluster_object
from centreseq.bin.tree.summary import read_summary_report
from centreseq.bin.tree.wrappers import call_muscle

main_log = logging.getLogger('main_log')


def tree_pipeline(summary_report: Path, prokka_dir: Path, outdir: Path, n_cpu: int, percentile: float, vcf_flag: bool):
    """
    Main pipeline call to extract data, (potentially call SNPs)
    :param vcf_flag: Flag to determine whether or not VCF files for clusters will be generated - DEPRECATED
    :param summary_report: Report generated by core pipeline (e.g. summary_report.tsv)
    :param prokka_dir: Directory containing all Prokka results generated through core pipeline
    :param outdir: Output directory
    :param n_cpu: Number of CPUs to allocate to tasks that support multithreading
    :param percentile: Filter summary report to the top nth percentile
    """
    # Initial reading a filtering of the summary report
    df = read_summary_report(summary_report)
    original_length = len(df)

    # Filter out singleton values
    df = df[df['n_members'] > 1]

    # Retrieve member list
    member_list = [x for x in df.columns.values if x not in
                   ['cluster', 'cluster_representative', 'product', 'n_members']]

    # Filter to nth percentile of samples
    n_members_max = np.percentile(df['n_members'], percentile)
    main_log.info(f"Filtering DataFrame to only contain rows where n_members >= {n_members_max}")
    df = df[df['n_members'] >= n_members_max]

    filtered_length = len(df)
    main_log.info(f"Extracting sequence information for core clusters described in {summary_report}")
    main_log.debug(f"Processing {filtered_length} clusters (filtered from {original_length} total clusters)")
    main_log.debug(
        f"{filtered_length}x{n_members_max} = {filtered_length * n_members_max} sequences will be extracted.")

    # Extract sequence data for each cluster member of every cluster (this is slow)
    tqdm.pandas()
    cluster_objects = df.progress_apply(populate_cluster_object, axis=1, args=(prokka_dir,))

    # Write extracted sequences to a file for each cluster
    main_log.info(f"Writing sequences to cluster files")
    aligned_loci_dir = outdir / 'aligned_loci'
    aligned_loci_dir.mkdir(exist_ok=True)
    for cluster_object in tqdm(cluster_objects, desc="Writing"):
        cluster_object.generate_cluster_fasta(outdir=aligned_loci_dir)

    # Align cluster multi-FASTA files
    main_log.info(f"Aligning all cluster files with Muscle")
    p = multiprocessing.Pool(processes=n_cpu)
    cluster_fastas = [cluster_object.cluster_fasta for cluster_object in cluster_objects]
    for _ in tqdm(p.imap_unordered(func=call_muscle, iterable=cluster_fastas), total=len(cluster_fastas),
                  desc="Muscle"):
        pass
